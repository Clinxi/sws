{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"YcGMcvZU_LuO"},"source":["# SWS3009 Lab 3 Introduction to Deep Learning\n","\n","\n","| group9   | name                |\n","|----------|---------------------|\n","| member1  | 贾世安(JIA SHIAN)    |\n","| member2  | 陶毅诚(TAO YICHENG)  |\n","\n","This lab should be done by both Deep Learning members of the team. Please ensure that you fill in the names of <b>both</b> team members in the spaces above. Answer <b>all</b> your questions on <b>this Python Notebook.</b>\n","\n","## Submission Instructions\n","\n","Please submit this Python notebook to Canvas on the deadline provided.\n","\n","Marks will be awarded as follows:\n","\n","**0 marks**: No/empty/Non-English submission\n","\n","**1 mark** : Poor submission\n","\n","**2 marks**: Acceptable submission\n","\n","**3 marks**: Good submission\n","\n","\n","## 1. Introduction\n","\n","We will achieve the following objectives in this lab:\n","\n","    1. An understanding of the practical limitations of using dense networks in complex tasks\n","    2. Hands-on experience in building a deep learning neural network to solve a relatively complex task.\n","    \n","\n","Each step may take a long time to run. You and your partner may want to work out how to do things simultaneously, but please do not miss out on any learning opportunities.\n","\n","\n","## 2. Submission Instructions\n","\n","Please submit your answer book to Canvas by the deadline.\n","\n","## 3. Creating a Dense Network for CIFAR-10\n","\n","We will now begin building a neural network for the CIFAR-10 dataset. The CIFAR-10 dataset consists of 50,000 32x32x3 (32x32 pixels, RGB channels) training images and 10,000 testing images (also 32x32x3), divided into the following 10 categories:\n","\n","    1. Airplane\n","    2. Automobile\n","    3. Bird\n","    4. Cat\n","    5. Deer\n","    6. Dog\n","    7. Frog\n","    8. Horse\n","    9. Ship\n","    10. Truck\n","    \n","In the first two parts of this lab we will create a classifier for the CIFAR-10 dataset.\n","\n","### 3.1 Loading the Dataset\n","\n","We begin firstly by creating a Dense neural network for CIFAR-10. The code below shows how we load the CIFAR-10 dataset:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20027,"status":"ok","timestamp":1688830583860,"user":{"displayName":"贾世安","userId":"17285649700372324292"},"user_tz":-480},"id":"qOwvMv_j_LuT","outputId":"af7da5c9-ec41-4fd9-a01d-b34d1d7ebd45"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 13s 0us/step\n"]}],"source":["from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.datasets import cifar10\n","\n","def load_cifar10():\n","    (train_x, train_y), (test_x, test_y) = cifar10.load_data()\n","    # reshape the image data from 3D arrays (32x32x3) to 2D arrays (3072 elements) to prepare them for further processing.\n","    train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n","    test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n","    # The image data (train_x and test_x) is then converted to float32 data type.\n","    train_x = train_x.astype('float32')\n","    test_x = test_x.astype('float32')\n","    # The pixel values of the image data are normalized by dividing them by 255.0\n","    # which scales the values between 0 and 1.\n","    train_x /= 255.0\n","    test_x /= 255.0\n","    # The labels (train_y and test_y) are one-hot encoded using to_categorical function from tensorflow.keras.utils.\n","    # This converts the label values from integers to binary vectors of size 10, representing the 10 classes in CIFAR-10.\n","    ret_train_y = to_categorical(train_y,10)\n","    ret_test_y = to_categorical(test_y, 10)\n","\n","    return (train_x, ret_train_y), (test_x, ret_test_y)\n","\n","\n","(train_x, train_y), (test_x, test_y) = load_cifar10()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UwajwaGCaun"},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YK2xFtQE_LuU"},"source":["----\n","\n","#### Question 1\n","\n","Explain what the following two  statements do, and where the number \"3072\" came from:\n","\n","```\n","  train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n","  test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n","```\n","\n","**Please put your answers in the attached answer books**\n","\n","what the following two  statements do: The two statements reshape the image data from 3D arrays of shape (32, 32, 3) to 2D arrays of shape (3072,)\n","\n","where the number \"3072\" came from: 3072 = 32x32x3, since the reshape function reshapes the image data from 3D arrays (32x32x3) to 2D arrays (3072 elements) to prepare them for further processing.\n","\n","\n","### 3.2 Building the MLP Classifier\n","\n","In the code box below, create a new fully connected (dense) multilayer perceptron classifier for the CIFAR-10 dataset. To begin with, create a network with one hidden layer of 1024 neurons, using the SGD optimizer. You should output the training and validation accuracy at every epoch, and train for 50 epochs:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"qK8AFjuH_LuV","outputId":"2fdcbea7-0f51-4754-c596-85da60aa1605"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training.\n","Epoch 1/50\n","1562/1563 [============================>.] - ETA: 0s - loss: 0.9812 - accuracy: 0.6552"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 60s 38ms/step - loss: 0.9812 - accuracy: 0.6551 - val_loss: 1.4126 - val_accuracy: 0.5347\n","Epoch 2/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.9671 - accuracy: 0.6578"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 57s 36ms/step - loss: 0.9671 - accuracy: 0.6578 - val_loss: 1.5234 - val_accuracy: 0.4951\n","Epoch 3/50\n","1562/1563 [============================>.] - ETA: 0s - loss: 0.9481 - accuracy: 0.6653"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 55s 35ms/step - loss: 0.9481 - accuracy: 0.6653 - val_loss: 1.5385 - val_accuracy: 0.4992\n","Epoch 4/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.9292 - accuracy: 0.6728"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 58s 37ms/step - loss: 0.9292 - accuracy: 0.6728 - val_loss: 1.4285 - val_accuracy: 0.5225\n","Epoch 5/50\n","1562/1563 [============================>.] - ETA: 0s - loss: 0.9156 - accuracy: 0.6792"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 61s 39ms/step - loss: 0.9156 - accuracy: 0.6792 - val_loss: 1.4766 - val_accuracy: 0.5165\n","Epoch 6/50\n","1562/1563 [============================>.] - ETA: 0s - loss: 0.8942 - accuracy: 0.6845"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 58s 37ms/step - loss: 0.8941 - accuracy: 0.6845 - val_loss: 1.4564 - val_accuracy: 0.5291\n","Epoch 7/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.8760 - accuracy: 0.6917"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 57s 36ms/step - loss: 0.8760 - accuracy: 0.6917 - val_loss: 1.4974 - val_accuracy: 0.5152\n","Epoch 8/50\n","1562/1563 [============================>.] - ETA: 0s - loss: 0.8651 - accuracy: 0.6952"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 59s 37ms/step - loss: 0.8651 - accuracy: 0.6952 - val_loss: 1.5070 - val_accuracy: 0.5219\n","Epoch 9/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.8462 - accuracy: 0.7031"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 56s 36ms/step - loss: 0.8462 - accuracy: 0.7031 - val_loss: 1.4724 - val_accuracy: 0.5348\n","Epoch 10/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.8339 - accuracy: 0.7071"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 58s 37ms/step - loss: 0.8339 - accuracy: 0.7071 - val_loss: 1.5132 - val_accuracy: 0.5227\n","Epoch 11/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.8122 - accuracy: 0.7127"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1563/1563 [==============================] - 55s 35ms/step - loss: 0.8122 - accuracy: 0.7127 - val_loss: 1.5118 - val_accuracy: 0.5265\n","Done. Now evaluating.\n","313/313 [==============================] - 3s 10ms/step - loss: 1.5118 - accuracy: 0.5265\n","Test accuracy: 0.53, loss: 1.51\n"]}],"source":["\"\"\"\n","Write your code to build an MLP with one hidden layer of 1024 neurons,\n","with an SGD optimizer. Train for 50 epochs, and output the training and\n","validation accuracy at each epoch.\n","\"\"\"\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","import os\n","\n","model_name = 'MLP_1'\n","\n","def BuildMLPModel(model_name):\n","    if os.path.exists(model_name):\n","        model = load_model(model_name)\n","    else:\n","        # The Sequential class is a linear stack of layers in Keras\n","        # which means you can easily add layers one by one.\n","        model = Sequential()\n","        # input layer\n","        #model.add(Dense(1 , activation='relu', input_shape = (3072, 1)))\n","\n","        # hidden layer\n","        # input_shape=(None, 1, 3072) 指定输入层的形状。这里的输入形状是(None, 1, 3072)，\n","        # 其中None表示可以接受任意数量的样本，1表示每个样本有一个维度，3072表示每个样本的特征维度为3072。\n","        model.add(Dense(1024, input_shape=(None, 1, 3072), activation='relu'))\n","        # output layer\n","        model = Dense(10, activation='softmax')(model)\n","        # model.add(Dense(10, activation='softmax'))\n","    return model\n","\n","\n","def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n","\n","    model.compile(optimizer=SGD(learning_rate=0.01, weight_decay = 1e-6, momentum=0.7),\n","                  loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    savemodel = ModelCheckpoint(model_name)\n","    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n","\n","    print(\"Starting training.\")\n","\n","    model.fit(x=train_x, y=train_y, batch_size=32,\n","    validation_data=(test_x, test_y), shuffle=True,\n","    epochs=epochs,\n","    callbacks=[savemodel, stopmodel])\n","\n","    print(\"Done. Now evaluating.\")\n","    loss, acc = model.evaluate(x=test_x, y=test_y)\n","    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))\n","\n","epochs = 50\n","model = BuildMLPModel(model_name)\n","train(model, train_x, train_y, epochs, test_x, test_y, model_name)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"--mcviml_LuV"},"source":["#### Question 2\n","\n","Complete the following table on the design choices for your MLP:\n","\n","| Hyperparameter       | What I used | Why?                  |\n","|:---------------------|:------------|:----------------------|\n","| Optimizer            | SGD         | Specified in question |\n","| # of hidden layers   | 1           | Specified in question |\n","| # of hidden neurons  | 1024        | Specified in question |\n","| Hid layer activation | relu        | it is normally used              |\n","| # of output neurons  | 10          | it is normally used              |\n","| Output activation    | softmax     | it is normally used              |\n","| lr                   | 0.01        | it is normally used              |\n","| momentum             | 0.7         | it is normally used              |\n","| decay                | 1e-6        | it is normally used              |\n","| loss                 |categorical_crossentropy| it is normally used              |\n","\n","\n","#### Question 3:\n","\n","What was your final training accuracy? Validation accuracy? Is there overfitting / underfitting? Explain your answer:\n","\n","***PLACE YOUR ANSWER HERE ***\n","final training accuracy is 0.7127, validation accuracy is 0.5265\n","there exists some overfitting since the validation accuracy is about 74% of the training accuracy\n","\n","### 3.3 Experimenting with the MLP\n","\n","Cut and paste your code from Section 3.2 to the box below (you may need to rename your MLP). Experiment with the number of hidden layers, the number of neurons in each hidden layer, the optimization algorithm, etc. See [Keras Optimizers](https://keras.io/optimizers) for the types of optimizers and their parameters. **Train for 100 epochs.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":624866,"status":"ok","timestamp":1688834724451,"user":{"displayName":"贾世安","userId":"17285649700372324292"},"user_tz":-480},"id":"Q7C2lpGT_LuV","outputId":"7e2e10b2-3902-4af6-e49c-a3138a6fb014"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training.\n","Epoch 1/100\n","1563/1563 [==============================] - 10s 6ms/step - loss: 2.1578 - accuracy: 0.2387 - val_loss: 2.0086 - val_accuracy: 0.2981\n","Epoch 2/100\n","1563/1563 [==============================] - 10s 6ms/step - loss: 1.9365 - accuracy: 0.3143 - val_loss: 1.8935 - val_accuracy: 0.3328\n","Epoch 3/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.8513 - accuracy: 0.3457 - val_loss: 1.8147 - val_accuracy: 0.3590\n","Epoch 4/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.7951 - accuracy: 0.3643 - val_loss: 1.7799 - val_accuracy: 0.3700\n","Epoch 5/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.7507 - accuracy: 0.3800 - val_loss: 1.7352 - val_accuracy: 0.3881\n","Epoch 6/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.7154 - accuracy: 0.3898 - val_loss: 1.6961 - val_accuracy: 0.3997\n","Epoch 7/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.6855 - accuracy: 0.4007 - val_loss: 1.6619 - val_accuracy: 0.4119\n","Epoch 8/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.6581 - accuracy: 0.4112 - val_loss: 1.6577 - val_accuracy: 0.4095\n","Epoch 9/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.6326 - accuracy: 0.4218 - val_loss: 1.6359 - val_accuracy: 0.4219\n","Epoch 10/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.6098 - accuracy: 0.4291 - val_loss: 1.5985 - val_accuracy: 0.4385\n","Epoch 11/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.5893 - accuracy: 0.4375 - val_loss: 1.5776 - val_accuracy: 0.4398\n","Epoch 12/100\n","1563/1563 [==============================] - 9s 5ms/step - loss: 1.5681 - accuracy: 0.4442 - val_loss: 1.5597 - val_accuracy: 0.4528\n","Epoch 13/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.5471 - accuracy: 0.4530 - val_loss: 1.5471 - val_accuracy: 0.4533\n","Epoch 14/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.5282 - accuracy: 0.4580 - val_loss: 1.5276 - val_accuracy: 0.4597\n","Epoch 15/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.5108 - accuracy: 0.4660 - val_loss: 1.5145 - val_accuracy: 0.4644\n","Epoch 16/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.4932 - accuracy: 0.4725 - val_loss: 1.5073 - val_accuracy: 0.4685\n","Epoch 17/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.4762 - accuracy: 0.4781 - val_loss: 1.4984 - val_accuracy: 0.4662\n","Epoch 18/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.4602 - accuracy: 0.4837 - val_loss: 1.4841 - val_accuracy: 0.4691\n","Epoch 19/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.4439 - accuracy: 0.4886 - val_loss: 1.4640 - val_accuracy: 0.4755\n","Epoch 20/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.4290 - accuracy: 0.4932 - val_loss: 1.4613 - val_accuracy: 0.4803\n","Epoch 21/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.4138 - accuracy: 0.4980 - val_loss: 1.4427 - val_accuracy: 0.4810\n","Epoch 22/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.3991 - accuracy: 0.5042 - val_loss: 1.4467 - val_accuracy: 0.4863\n","Epoch 23/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.3849 - accuracy: 0.5105 - val_loss: 1.4450 - val_accuracy: 0.4862\n","Epoch 24/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.3724 - accuracy: 0.5151 - val_loss: 1.4342 - val_accuracy: 0.4929\n","Epoch 25/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.3568 - accuracy: 0.5191 - val_loss: 1.4257 - val_accuracy: 0.4887\n","Epoch 26/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.3454 - accuracy: 0.5228 - val_loss: 1.4060 - val_accuracy: 0.5003\n","Epoch 27/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.3294 - accuracy: 0.5306 - val_loss: 1.4147 - val_accuracy: 0.4986\n","Epoch 28/100\n","1563/1563 [==============================] - 10s 7ms/step - loss: 1.3187 - accuracy: 0.5315 - val_loss: 1.4278 - val_accuracy: 0.4910\n","Epoch 29/100\n","1563/1563 [==============================] - 10s 6ms/step - loss: 1.3055 - accuracy: 0.5353 - val_loss: 1.3902 - val_accuracy: 0.5048\n","Epoch 30/100\n","1563/1563 [==============================] - 10s 6ms/step - loss: 1.2943 - accuracy: 0.5430 - val_loss: 1.3825 - val_accuracy: 0.5102\n","Epoch 31/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.2824 - accuracy: 0.5457 - val_loss: 1.3826 - val_accuracy: 0.5080\n","Epoch 32/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.2710 - accuracy: 0.5501 - val_loss: 1.3847 - val_accuracy: 0.5089\n","Epoch 33/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.2590 - accuracy: 0.5529 - val_loss: 1.3858 - val_accuracy: 0.5141\n","Epoch 34/100\n","1563/1563 [==============================] - 10s 6ms/step - loss: 1.2486 - accuracy: 0.5581 - val_loss: 1.4133 - val_accuracy: 0.5015\n","Epoch 35/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.2371 - accuracy: 0.5607 - val_loss: 1.3655 - val_accuracy: 0.5156\n","Epoch 36/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.2280 - accuracy: 0.5647 - val_loss: 1.3485 - val_accuracy: 0.5164\n","Epoch 37/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.2144 - accuracy: 0.5695 - val_loss: 1.3644 - val_accuracy: 0.5110\n","Epoch 38/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.2040 - accuracy: 0.5730 - val_loss: 1.3552 - val_accuracy: 0.5204\n","Epoch 39/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.1936 - accuracy: 0.5770 - val_loss: 1.3605 - val_accuracy: 0.5153\n","Epoch 40/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.1813 - accuracy: 0.5826 - val_loss: 1.3399 - val_accuracy: 0.5265\n","Epoch 41/100\n","1563/1563 [==============================] - 9s 5ms/step - loss: 1.1722 - accuracy: 0.5858 - val_loss: 1.3590 - val_accuracy: 0.5160\n","Epoch 42/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.1591 - accuracy: 0.5887 - val_loss: 1.3502 - val_accuracy: 0.5197\n","Epoch 43/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.1512 - accuracy: 0.5937 - val_loss: 1.4025 - val_accuracy: 0.5071\n","Epoch 44/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.1409 - accuracy: 0.5971 - val_loss: 1.3329 - val_accuracy: 0.5270\n","Epoch 45/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.1320 - accuracy: 0.5989 - val_loss: 1.3335 - val_accuracy: 0.5254\n","Epoch 46/100\n","1563/1563 [==============================] - 9s 5ms/step - loss: 1.1173 - accuracy: 0.6033 - val_loss: 1.3357 - val_accuracy: 0.5306\n","Epoch 47/100\n","1563/1563 [==============================] - 9s 5ms/step - loss: 1.1106 - accuracy: 0.6062 - val_loss: 1.3573 - val_accuracy: 0.5229\n","Epoch 48/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.1001 - accuracy: 0.6118 - val_loss: 1.3659 - val_accuracy: 0.5205\n","Epoch 49/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.0889 - accuracy: 0.6145 - val_loss: 1.3280 - val_accuracy: 0.5351\n","Epoch 50/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.0802 - accuracy: 0.6180 - val_loss: 1.3565 - val_accuracy: 0.5262\n","Epoch 51/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.0732 - accuracy: 0.6208 - val_loss: 1.3569 - val_accuracy: 0.5237\n","Epoch 52/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.0587 - accuracy: 0.6269 - val_loss: 1.3956 - val_accuracy: 0.5143\n","Epoch 53/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.0511 - accuracy: 0.6284 - val_loss: 1.3908 - val_accuracy: 0.5228\n","Epoch 54/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.0407 - accuracy: 0.6333 - val_loss: 1.3794 - val_accuracy: 0.5161\n","Epoch 55/100\n","1563/1563 [==============================] - 9s 5ms/step - loss: 1.0301 - accuracy: 0.6363 - val_loss: 1.3264 - val_accuracy: 0.5338\n","Epoch 56/100\n","1563/1563 [==============================] - 9s 6ms/step - loss: 1.0183 - accuracy: 0.6416 - val_loss: 1.3591 - val_accuracy: 0.5278\n","Epoch 57/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.0097 - accuracy: 0.6428 - val_loss: 1.3601 - val_accuracy: 0.5286\n","Epoch 58/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.0030 - accuracy: 0.6457 - val_loss: 1.3396 - val_accuracy: 0.5346\n","Epoch 59/100\n","1563/1563 [==============================] - 11s 7ms/step - loss: 0.9902 - accuracy: 0.6508 - val_loss: 1.3607 - val_accuracy: 0.5330\n","Epoch 60/100\n","1563/1563 [==============================] - 12s 8ms/step - loss: 0.9764 - accuracy: 0.6555 - val_loss: 1.3698 - val_accuracy: 0.5327\n","Epoch 61/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.9726 - accuracy: 0.6574 - val_loss: 1.3883 - val_accuracy: 0.5228\n","Epoch 62/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.9597 - accuracy: 0.6633 - val_loss: 1.3887 - val_accuracy: 0.5318\n","Epoch 63/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.9507 - accuracy: 0.6630 - val_loss: 1.4014 - val_accuracy: 0.5199\n","Epoch 64/100\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.9394 - accuracy: 0.6696 - val_loss: 1.3605 - val_accuracy: 0.5337\n","Epoch 65/100\n","1563/1563 [==============================] - 9s 5ms/step - loss: 0.9287 - accuracy: 0.6735 - val_loss: 1.4042 - val_accuracy: 0.5296\n","Done. Now evaluating.\n","313/313 [==============================] - 1s 3ms/step - loss: 1.4042 - accuracy: 0.5296\n","Test accuracy: 0.53, loss: 1.40\n"]}],"source":["\"\"\"\n","Cut and paste your code from Section 3.2 below, then modify it to get\n","much better results than what you had earlier. E.g. increase the number of\n","nodes in the hidden layer, increase the number of hidden layers,\n","change the optimizer, etc.\n","\n","Train for 100 epochs.\n","\n","\"\"\"\n","\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","import os\n","\n","model_name = 'MLP_2'\n","\n","def BuildMLPModel(model_name):\n","    if os.path.exists(model_name):\n","        model = load_model(model_name)\n","    else:\n","        # The Sequential class is a linear stack of layers in Keras\n","        # which means you can easily add layers one by one.\n","        model = Sequential()\n","        # input layer\n","        #model.add(Dense(1 , activation='relu', input_shape = (3072, 1)))\n","\n","        # hidden layer\n","        # input_shape=(None, 1, 3072) 指定输入层的形状。这里的输入形状是(None, 1, 3072)，\n","        # 其中None表示可以接受任意数量的样本，1表示每个样本有一个维度，3072表示每个样本的特征维度为3072。\n","        model.add(Dense(1024, input_shape=(None, 1, 3072), activation='relu'))\n","        model.add(Dense(32, activation='relu'))\n","        model.add(Dense(512, activation='relu'))\n","        # output layer\n","        model.add(Dense(10, activation='softmax'))\n","        # model.add(Dense(10, activation='softmax'))\n","    return model\n","\n","\n","def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n","\n","    model.compile(optimizer=SGD(learning_rate=0.0005, weight_decay = 0.00001, momentum=0.7),\n","                  loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    savemodel = ModelCheckpoint(model_name)\n","    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n","\n","    print(\"Starting training.\")\n","\n","    model.fit(x=train_x, y=train_y, batch_size=32,\n","    validation_data=(test_x, test_y), shuffle=True,\n","    epochs=epochs,\n","    callbacks=[savemodel, stopmodel])\n","\n","    print(\"Done. Now evaluating.\")\n","    loss, acc = model.evaluate(x=test_x, y=test_y)\n","    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))\n","\n","epochs = 100\n","model = BuildMLPModel(model_name)\n","train(model, train_x, train_y, epochs, test_x, test_y, model_name)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gfIwFpbQ_LuW"},"source":["----\n","\n","#### Question 4:\n","\n","Complete the following table with your final design (you may add more rows for the # neurons (layer1) etc. to detail how many neurons you have in each hidden layer). Likewise you may replace the lr, momentum etc rows with parameters more appropriate to the optimizer that you have chosen.\n","\n","\n","| Hyperparameter       | What I used | Why?                  |\n","|:---------------------|:------------|:----------------------|\n","| Optimizer            |SGD          |it is fast and also useful|\n","| # of hidden layers   |3            |3 is ok                |\n","| # neurons(layer1)    |1024         |it is ok               |\n","| Hid layer1 activation|relu         |  useful               |\n","| # neurons(layer2)    |32           |just try               |\n","| Hid layer2 activation|relu         |it is ok               |\n","| # neurons(layer2)    |512          |just try               |\n","| Hid layer2 activation|relu         |it is ok               |\n","| # of output neurons  |10           |we have 10 classes     |\n","| Output activation    |softmax      |it is a classification problem|\n","| lr                   |0.0005       |try many times, it is not so bad|\n","| momentum             |0.7          |try many times, it is not so bad|\n","| decay                |0.00001      |try many times, it is not so bad|\n","| loss                 |categorical_crossentropy|it is ok         |\n","\n","\n","\n","#### Question 5\n","\n","What is the final training and validation accuracy that you obtained after 150 epochs. Is there considerable improvement over Section 3.2? Are there still signs of underfitting or overfitting? Explain your answer.\n","\n","***Write your answers here***\n","there is no considerable improvement over section 3.2, there still exists some sign of overfitting, since the training acc is 0.67 and the validation acc is 0.53\n","\n","#### Question 6\n","\n","Write a short reflection on the practical difficulties of using a dense MLP to classsify images in the CIFAR-10 datasets.\n","\n","***Write your answers here***\n","the result of using a dense MLP to classify images in the CIFAR-10 datasets is not very good, and the results exists sign of overfitting, although we have tried many differnet kinds of parameters, the result has no big difference\n","\n","----\n","\n","## 4. Creating a CNN for the MNIST Data Set\n","\n","In this section we will now create a convolutional neural network (CNN) to classify images in the MNIST dataset that we used in the previous lab. Let's go through each part to see how to do this.\n","\n","### 4.1 Loading the MNIST Dataset\n","\n","As always we will load the MNIST dataset, scale the inputs to between 0 and 1, and convert the Y labels to one-hot vectors. However unlike before we will not flatten the 28x28 image to a 784 element vector, since CNNs can inherently handle 2D data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wcu8qe2_LuW"},"outputs":[],"source":["from keras.datasets import mnist\n","from keras.utils import to_categorical\n","\n","def load_mnist():\n","    (train_x, train_y),(test_x, test_y) = mnist.load_data()\n","    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n","    test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n","\n","    train_x=train_x.astype('float32')\n","    test_x = test_x.astype('float32')\n","\n","    train_x /= 255.0\n","    test_x /= 255.0\n","\n","    train_y = to_categorical(train_y, 10)\n","    test_y = to_categorical(test_y, 10)\n","\n","    return (train_x, train_y), (test_x, test_y)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZEcVbv7r_LuW"},"source":["### 4.2 Building the CNN\n","\n","We will now build the CNN. Unlike before we will create a function to produce the CNN. We will also look at how to save and load Keras models using \"checkpoints\", particularly \"ModelCheckpoint\" that saves the model each epoch.\n","\n","Let's begin by creating the model. We call os.path.exists to see if a model file exists, and call \"load_model\" if it does. Otherwise we create a new model.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fENvivV_LuW"},"outputs":[],"source":["# load_model loads a model from a hd5 file.\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n","import os\n","\n","MODEL_NAME = 'mnist-cnn.hd5'\n","\n","def buildmodel(model_name):\n","    if os.path.exists(model_name):\n","        model = load_model(model_name)\n","    else:\n","        model = Sequential()\n","        model.add(Conv2D(32, kernel_size=(5,5),\n","        activation='relu',\n","        input_shape=(28, 28, 1), padding='same')) # Question 7\n","\n","        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n","        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n","        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n","        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n","        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n","        model.add(Flatten()) # Question 9\n","        model.add(Dense(1024, activation='relu'))\n","        model.add(Dropout(0.1))\n","        model.add(Dense(10, activation='softmax'))\n","\n","    return model\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hnfU74ST_LuX"},"source":["----\n","\n","#### Question 7\n","\n","The first layer in our CNN is a 2D convolution kernel, shown here:\n","\n","```\n","        model.add(Conv2D(32, kernel_size=(5,5),\n","        activation='relu',\n","        input_shape=(28, 28, 1), padding='same')) # Question 7\n","```\n","\n","Why is the input_shape set to (28, 28, 1)? What does this mean? What does \"padding = 'same'\" mean?\n","\n","***Write your answer here***\n","`input_shape=(28, 28, 1)` specifies the shape of the input data. The first two dimensions (28, 28) represent the height and width of the input image, while the last dimension 1 indicates a single channel, representing grayscale images.\n","\n","`padding='same'` determines the padding strategy for the convolutional layer. 'same' padding means that the output feature maps will have the same spatial dimensions as the input feature maps. And the input image is padded with zeros on the borders if necessary to maintain the same spatial dimensions. This ensures that the output feature maps have the same height and width as the input feature maps.\n","\n","#### Question 8\n","\n","The second layer is the MaxPooling2D layer shown below:\n","\n","```\n","        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n","```\n","\n","What other types of pooling layers are available? What does 'strides = 2' mean?\n","\n","***Write your answer here***\n","1. other types of pooling layers that are available including AveragePooling2D、GlobalAveragePooling2D、GlobalMaxPooling2D and so on\n","2. `strides=2` means that the pooling window moves by 2 units horizontally and vertically.\n","\n","#### Question 9\n","\n","What does the \"Flatten\" layer here do? Why is it needed?\n","\n","```\n","        model.add(Flatten()) # Question 9\n","```\n","\n","***Write your answer here***\n","In a CNN, the earlier layers typically consist of convolutional and pooling layers that preserve the spatial structure of the input data. However, the subsequent layers are often fully connected layers that expect a 1D vector as input. The Flatten layer bridges the gap between the convolutional layers and the fully connected layers by flattening the output of the previous layers into a 1D vector, by flattening the output, the subsequent fully connected layers can process the data as a traditional feedforward neural network. These fully connected layers are typically responsible for learning higher-level abstractions and making predictions.\n","\n","\n","----\n","\n","### 4.3 Training the CNN\n","\n","Let's now train the CNN. In this example we introduce the idea of a \"callback\", which is a routine that Keras calls at the end of each epoch. Specifically we look at two callbacks:\n","\n","    1. ModelCheckpoint: When called, Keras saves the model to the specified filename.\n","    \n","    2. EarlyStopping: When called, Keras checks if it should stop the training prematurely.\n","    \n","\n","Let's look at the code to see how training is done, and how callbacks are used."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sE-HI6Hp_LuX"},"outputs":[],"source":["from keras.optimizers import SGD\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n","\n","    model.compile(optimizer=SGD(lr=0.01, momentum=0.7),\n","                  loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    savemodel = ModelCheckpoint(model_name)\n","    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n","\n","    print(\"Starting training.\")\n","\n","    model.fit(x=train_x, y=train_y, batch_size=32,\n","    validation_data=(test_x, test_y), shuffle=True,\n","    epochs=epochs,\n","    callbacks=[savemodel, stopmodel])\n","\n","    print(\"Done. Now evaluating.\")\n","    loss, acc = model.evaluate(x=test_x, y=test_y)\n","    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fQdf7v1U_LuX"},"source":["Notice that there isn't very much that is unusual going on; we compile the model with our loss function and optimizer, then call fit, and finally evaluate to look at the final accuracy for the test set.  The only thing unusual is the \"callbacks\" parameter here in the fit function call\n","\n","```\n","    model.fit(x=train_x, y=train_y, batch_size=32,\n","    validation_data=(test_x, test_y), shuffle=True,\n","    epochs=epochs,\n","    callbacks=[savemodel, stopmodel])\n","```\n","\n","----\n","\n","#### Question 10.\n","\n","What do the min_delta and patience parameters do in the EarlyStopping callback, as shown below? (2 MARKS)\n","\n","```\n","    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n","```\n","\n","`min_delta`: The min_delta parameter specifies the minimum change in the monitored quantity that is considered as an improvement. If the improvement in the monitored quantity is less than min_delta, it is not considered significant, and the training process continues. On the other hand, if the improvement is greater than or equal to min_delta, it is considered significant, and the training continues. The min_delta value is typically set based on the desired sensitivity to changes and the scale of the monitored quantity.\n","\n","`patience`: The patience parameter determines the number of epochs to wait before stopping the training process if there is no significant improvement in the monitored quantity. It measures the number of epochs with no improvement before the training is halted. If, after patience epochs, the monitored quantity does not improve by at least min_delta, the training process is stopped early\n","\n","---\n","\n","### 4.4 Putting it together.\n","\n","Now let's run the code and see how it goes (Note: To save time we are training for only 5 epochs; we should train much longer to get much better results):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88058,"status":"ok","timestamp":1688882034978,"user":{"displayName":"贾世安","userId":"17285649700372324292"},"user_tz":-480},"id":"XJq__3Ly_LuY","outputId":"8c48a2c7-6e3b-45f2-b5bf-91623dfba45e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Starting training.\n","Epoch 1/5\n","1874/1875 [============================>.] - ETA: 0s - loss: 0.3568 - accuracy: 0.8867"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 19s 6ms/step - loss: 0.3567 - accuracy: 0.8868 - val_loss: 0.0793 - val_accuracy: 0.9734\n","Epoch 2/5\n","1874/1875 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9768"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 11s 6ms/step - loss: 0.0750 - accuracy: 0.9768 - val_loss: 0.0744 - val_accuracy: 0.9761\n","Epoch 3/5\n","1863/1875 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9848"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 10s 5ms/step - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0501 - val_accuracy: 0.9842\n","Epoch 4/5\n","1864/1875 [============================>.] - ETA: 0s - loss: 0.0361 - accuracy: 0.9884"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 11s 6ms/step - loss: 0.0362 - accuracy: 0.9883 - val_loss: 0.0347 - val_accuracy: 0.9891\n","Epoch 5/5\n","1868/1875 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9914"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["1875/1875 [==============================] - 10s 5ms/step - loss: 0.0285 - accuracy: 0.9914 - val_loss: 0.0356 - val_accuracy: 0.9887\n","Done. Now evaluating.\n","313/313 [==============================] - 1s 4ms/step - loss: 0.0356 - accuracy: 0.9887\n","Test accuracy: 0.99, loss: 0.04\n"]}],"source":["    (train_x, train_y),(test_x, test_y) = load_mnist()\n","    model = buildmodel(MODEL_NAME)\n","    train(model, train_x, train_y, 5, test_x, test_y, MODEL_NAME)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"o1lxLdSc_LuY"},"source":["----\n","\n","#### Question 11.\n","\n","Compare the relative advantages and disadvantages of CNN vs. the Dense MLP that you build in sections 3.2 and 3.3. What makes CNNs better (or worse)?\n","\n","***Type your answers here***\n","+ Advantages of CNNs to make CNNs better:\n","1. Spatial feature extraction: CNNs are designed to efficiently capture spatial relationships and extract meaningful features from images or other grid-like data. The use of convolutional layers with shared weights enables the model to identify local patterns and hierarchically learn more complex representations.\n","2. Translation invariance: CNNs are able to recognize patterns irrespective of their location in the input. This translation invariance property makes CNNs robust to spatial transformations, such as image translations, rotations, and scale changes\n","3. Parameter sharing: CNNs have a parameter sharing mechanism, where the same set of weights is applied to different parts of the input. This sharing of parameters helps reduce the number of parameters and makes the model more efficient and effective in learning from limited data.\n","4. Hierarchical representation learning: CNNs can learn hierarchical representations of the input data by stacking multiple convolutional and pooling layers. Lower layers learn low-level features like edges and textures, while higher layers learn more abstract and high-level representations.\n","5. Reduced overfitting: The pooling and down-sampling operations in CNNs help reduce the spatial dimensions of the feature maps, which can prevent overfitting by providing a form of regularization. Additionally, the use of dropout and regularization techniques in CNNs further aids in reducing overfitting.\n","\n","+ Disadvantages of CNNs to make CNNs worse:\n","\n","1. Limited interpretability: Due to their complex architecture and hierarchical feature extraction, interpreting the learned features and understanding the decision-making process in CNNs can be challenging. Dense MLPs, with their fully connected layers, provide more direct interpretability.\n","\n","## 5. Making a CNN for the CIFAR-10 Dataset\n","\n","Now comes the fun part: Using the example above for creating a CNN for the MNIST dataset, now create a CNN in the box below for the CIFAR-10 dataset. At the end of each epoch save the model to a file called \"cifar.hd5\" (note: the .hd5 is added automatically for you).\n","\n","---\n","\n","#### Question 12.\n","\n","Summarize your design in the table below (the actual coding cell comes after this):\n","\n","model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3), padding='same'))\n","        model.add(BatchNormalization())  # Adding Batch Normalization\n","        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Flatten())\n","        model.add(Dense(512, activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(0.5))\n","        model.add(Dense(10, activation='softmax'))\n","\n","\n","| Hyperparameter       | What I used | Why?                  |\n","|:---------------------|:------------|:----------------------|\n","| Optimizer            |SGD          |it's ok compared with other kinds of optimizer    |\n","| Input shape          |32x32x3      |The CIFAR-10 dataset consists of color images,<br> where each image has a height and width of 32 pixels. |\n","| First layer          |Conv2D       |just have a try        |\n","| Second layer         |MaxPooling2D |just have a try        |\n","| Add more layers      |Conv2D       |just have a try        |\n","| Add more layers      |Conv2D       |just have a try        |\n","| Add more layers      |MaxPooling2D |just have a try        |\n","| Add more layers      |Flatten      |just have a try        |\n","| Dense layer          |Dense(512, activation='relu')|just have a try        |\n","|Output layer          |Dense(10, activation='softmax')|use softmax for 10 classes|\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507032,"status":"ok","timestamp":1688884453943,"user":{"displayName":"贾世安","userId":"17285649700372324292"},"user_tz":-480},"id":"yxAqU1E-_LuY","outputId":"6f17a446-e94b-43a0-a61a-cf90cd80bfc9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Starting training.\n","Epoch 1/50\n","1563/1563 [==============================] - ETA: 0s - loss: 1.6189 - accuracy: 0.4525"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 16s 9ms/step - loss: 1.6189 - accuracy: 0.4525 - val_loss: 1.2041 - val_accuracy: 0.5705\n","Epoch 2/50\n","1559/1563 [============================>.] - ETA: 0s - loss: 1.1705 - accuracy: 0.5906"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 10ms/step - loss: 1.1707 - accuracy: 0.5906 - val_loss: 0.9338 - val_accuracy: 0.6705\n","Epoch 3/50\n","1561/1563 [============================>.] - ETA: 0s - loss: 1.0153 - accuracy: 0.6461"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 9ms/step - loss: 1.0151 - accuracy: 0.6461 - val_loss: 0.9176 - val_accuracy: 0.6826\n","Epoch 4/50\n","1557/1563 [============================>.] - ETA: 0s - loss: 0.9103 - accuracy: 0.6822"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.9100 - accuracy: 0.6823 - val_loss: 0.8572 - val_accuracy: 0.7016\n","Epoch 5/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.8489 - accuracy: 0.7055"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.8489 - accuracy: 0.7055 - val_loss: 0.7746 - val_accuracy: 0.7280\n","Epoch 6/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.7904 - accuracy: 0.7246"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 9ms/step - loss: 0.7904 - accuracy: 0.7246 - val_loss: 0.6833 - val_accuracy: 0.7626\n","Epoch 7/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.7453 - accuracy: 0.7400"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 10ms/step - loss: 0.7453 - accuracy: 0.7400 - val_loss: 0.6977 - val_accuracy: 0.7521\n","Epoch 8/50\n","1559/1563 [============================>.] - ETA: 0s - loss: 0.7079 - accuracy: 0.7534"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.7080 - accuracy: 0.7534 - val_loss: 0.7304 - val_accuracy: 0.7476\n","Epoch 9/50\n","1561/1563 [============================>.] - ETA: 0s - loss: 0.6679 - accuracy: 0.7651"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 10ms/step - loss: 0.6677 - accuracy: 0.7652 - val_loss: 0.7006 - val_accuracy: 0.7632\n","Epoch 10/50\n","1561/1563 [============================>.] - ETA: 0s - loss: 0.6333 - accuracy: 0.7783"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.6334 - accuracy: 0.7783 - val_loss: 0.6785 - val_accuracy: 0.7697\n","Epoch 11/50\n","1559/1563 [============================>.] - ETA: 0s - loss: 0.6094 - accuracy: 0.7869"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.6090 - accuracy: 0.7871 - val_loss: 0.6156 - val_accuracy: 0.7890\n","Epoch 12/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.5743 - accuracy: 0.7993"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.5743 - accuracy: 0.7993 - val_loss: 0.6291 - val_accuracy: 0.7874\n","Epoch 13/50\n","1561/1563 [============================>.] - ETA: 0s - loss: 0.5552 - accuracy: 0.8044"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 16s 10ms/step - loss: 0.5553 - accuracy: 0.8045 - val_loss: 0.7562 - val_accuracy: 0.7495\n","Epoch 14/50\n","1560/1563 [============================>.] - ETA: 0s - loss: 0.5293 - accuracy: 0.8141"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.5296 - accuracy: 0.8139 - val_loss: 0.6272 - val_accuracy: 0.7879\n","Epoch 15/50\n","1561/1563 [============================>.] - ETA: 0s - loss: 0.5035 - accuracy: 0.8223"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.5036 - accuracy: 0.8223 - val_loss: 0.5761 - val_accuracy: 0.8028\n","Epoch 16/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.4848 - accuracy: 0.8299"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.4848 - accuracy: 0.8299 - val_loss: 0.5652 - val_accuracy: 0.8060\n","Epoch 17/50\n","1562/1563 [============================>.] - ETA: 0s - loss: 0.4659 - accuracy: 0.8345"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.4660 - accuracy: 0.8345 - val_loss: 0.5597 - val_accuracy: 0.8154\n","Epoch 18/50\n","1560/1563 [============================>.] - ETA: 0s - loss: 0.4512 - accuracy: 0.8423"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.4516 - accuracy: 0.8422 - val_loss: 0.5997 - val_accuracy: 0.8060\n","Epoch 19/50\n","1560/1563 [============================>.] - ETA: 0s - loss: 0.4314 - accuracy: 0.8470"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 9ms/step - loss: 0.4312 - accuracy: 0.8471 - val_loss: 0.5630 - val_accuracy: 0.8112\n","Epoch 20/50\n","1562/1563 [============================>.] - ETA: 0s - loss: 0.4171 - accuracy: 0.8534"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.4174 - accuracy: 0.8533 - val_loss: 0.5998 - val_accuracy: 0.7994\n","Epoch 21/50\n","1561/1563 [============================>.] - ETA: 0s - loss: 0.4039 - accuracy: 0.8560"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.4043 - accuracy: 0.8559 - val_loss: 0.5800 - val_accuracy: 0.8128\n","Epoch 22/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.3900 - accuracy: 0.8618"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 9ms/step - loss: 0.3900 - accuracy: 0.8618 - val_loss: 0.5449 - val_accuracy: 0.8221\n","Epoch 23/50\n","1561/1563 [============================>.] - ETA: 0s - loss: 0.3743 - accuracy: 0.8652"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 9ms/step - loss: 0.3745 - accuracy: 0.8651 - val_loss: 0.5239 - val_accuracy: 0.8254\n","Epoch 24/50\n","1556/1563 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8725"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 10ms/step - loss: 0.3579 - accuracy: 0.8725 - val_loss: 0.5567 - val_accuracy: 0.8212\n","Epoch 25/50\n","1560/1563 [============================>.] - ETA: 0s - loss: 0.3555 - accuracy: 0.8738"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.3555 - accuracy: 0.8738 - val_loss: 0.5400 - val_accuracy: 0.8254\n","Epoch 26/50\n","1560/1563 [============================>.] - ETA: 0s - loss: 0.3378 - accuracy: 0.8797"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.3377 - accuracy: 0.8798 - val_loss: 0.5430 - val_accuracy: 0.8307\n","Epoch 27/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.3275 - accuracy: 0.8844"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 9ms/step - loss: 0.3275 - accuracy: 0.8844 - val_loss: 0.5948 - val_accuracy: 0.8157\n","Epoch 28/50\n","1557/1563 [============================>.] - ETA: 0s - loss: 0.3248 - accuracy: 0.8849"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 9ms/step - loss: 0.3251 - accuracy: 0.8847 - val_loss: 0.6402 - val_accuracy: 0.8008\n","Epoch 29/50\n","1556/1563 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8922"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.3060 - accuracy: 0.8921 - val_loss: 0.5580 - val_accuracy: 0.8248\n","Epoch 30/50\n","1556/1563 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8942"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 10ms/step - loss: 0.2970 - accuracy: 0.8942 - val_loss: 0.5368 - val_accuracy: 0.8300\n","Epoch 31/50\n","1563/1563 [==============================] - ETA: 0s - loss: 0.2940 - accuracy: 0.8955"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 9ms/step - loss: 0.2940 - accuracy: 0.8955 - val_loss: 0.5382 - val_accuracy: 0.8361\n","Epoch 32/50\n","1556/1563 [============================>.] - ETA: 0s - loss: 0.2840 - accuracy: 0.8976"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 14s 9ms/step - loss: 0.2843 - accuracy: 0.8976 - val_loss: 0.6088 - val_accuracy: 0.8173\n","Epoch 33/50\n","1559/1563 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8999"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 15s 10ms/step - loss: 0.2784 - accuracy: 0.8999 - val_loss: 0.5501 - val_accuracy: 0.8293\n","Done. Now evaluating.\n","313/313 [==============================] - 1s 4ms/step - loss: 0.5501 - accuracy: 0.8293\n","Test accuracy: 0.83, loss: 0.55\n"]}],"source":["\"\"\"\n","Write your code for your CNN for the CIFAR-10 dataset here.\n","\n","Note: train_x, train_y, test_x, test_y were changed when we called\n","load_mnist in the previous section. You will now need to call load_cifar10\n","again.\n","\n","\"\"\"\n","\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.datasets import cifar10\n","\n","def load_cifar10_1():\n","    (train_x, train_y),(test_x, test_y) = cifar10.load_data()\n","    train_x = train_x.reshape(train_x.shape[0], 32, 32, 3)\n","    test_x = test_x.reshape(test_x.shape[0], 32, 32, 3)\n","\n","    train_x=train_x.astype('float32')\n","    test_x = test_x.astype('float32')\n","\n","    train_x /= 255.0\n","    test_x /= 255.0\n","\n","    train_y = to_categorical(train_y, 10)\n","    test_y = to_categorical(test_y, 10)\n","\n","    return (train_x, train_y), (test_x, test_y)\n","\n","\n","\n","# load_model loads a model from a hd5 file.\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n","import os\n","\n","MODEL_NAME = 'mnist-cnn_2.hd5'\n","\n","\n","from tensorflow.keras.layers import Dropout, BatchNormalization\n","\n","def buildmodel(model_name):\n","    if os.path.exists(model_name):\n","        model = load_model(model_name)\n","    else:\n","        model = Sequential()\n","        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3), padding='same'))\n","        model.add(BatchNormalization())  # Adding Batch Normalization\n","        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Flatten())\n","        model.add(Dense(512, activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(0.5))\n","        model.add(Dense(10, activation='softmax'))\n","\n","    return model\n","\n","\n","\n","\n","\n","from keras.optimizers import SGD\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n","\n","    model.compile(optimizer=SGD(lr=0.01, momentum=0.7),\n","                  loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    savemodel = ModelCheckpoint(model_name)\n","    stopmodel = EarlyStopping(min_delta=0.001, patience=10)\n","\n","    print(\"Starting training.\")\n","\n","    model.fit(x=train_x, y=train_y, batch_size=32,\n","    validation_data=(test_x, test_y), shuffle=True,\n","    epochs=epochs,\n","    callbacks=[savemodel, stopmodel])\n","\n","    print(\"Done. Now evaluating.\")\n","    loss, acc = model.evaluate(x=test_x, y=test_y)\n","    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))\n","\n","\n","epoch = 50\n","(train_x, train_y), (test_x, test_y) = load_cifar10_1()\n","model = buildmodel(MODEL_NAME)\n","train(model, train_x, train_y, epoch, test_x, test_y, MODEL_NAME)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BENA-X17_LuZ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
